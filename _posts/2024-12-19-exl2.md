---
title: "Minimal set-up for LLMs quantized in EXLV2 format."
categories:
  - Machine-Learning
tags:
  - machine learning
  - deep learning
  - LLM
show_date: true
---

I have been working on a side project with an LLM integration, which someday I will share more about. For this side project, I have been experimenting with various LLM-quantization formats and found the Qwen 2.5 14B quantized to 8 bits to be a sweet spot for speed and performance on my 3090 RTX. With the ExLlamaV2 format, I can achieve throughput of of 40-55 token/second with an context window of 1-2 thousand tokens on my 3090 RTX (I have not needed to context windows any larger). The performance is really impressive once nailing down the generation parameters, although occasionally the model will start burping Chinese characters. [Zoyd's quantized version](https://huggingface.co/Zoyd/Qwen_Qwen2.5-14B-Instruct-8_0bpw_exl2) is the model I have deployed lately.

I may create a separate post comparing EXLV2 to other popular formats like GGUF on my hardware, but I have decided to share the source code I am using for an API endpoint since options are limited. [tabbyAPI](https://github.com/theroyallab/tabbyAPI/) is a nice option, but I wanted something more customized to my needs. As a design decision, I only needed a single response and am not using the HuggingFace chat template, although I plan on adding this option in the future.

Here is a screenshot of a sample response.

<figure class="align-center">
  <img src="/assets/images/exlv2_server.png" alt="exlv2 server screenshot">
</figure>

For complete details on getting set-up, refer to the project home page at [https://github.com/Tim-Roy/exl2-server.git](https://github.com/Tim-Roy/exl2-server.git). To get started you will need:

- An NVIDA GPU with sufficient RAM
- Linux environment
- [ExLlamaV2](https://github.com/turboderp/exllamav2)
- [flash attention](https://github.com/Dao-AILab/flash-attention)
- [An ExLlamaV2 quantized LLM](https://huggingface.co/models?search=exlv2)

Once installed, set the environment variable `MODEL_HOME`, the path to the parent directory of the models you wish to deploy.

Next, copy the `models.yaml` configuration file to `MODEL_HOME` and make any necessary changes.

Then, set `EXL2_MODEL` to one of the keys in `models.yaml`.

Finally deploy the server using uvicorn (for example, `uvicorn exl2.server:app --host 0.0.0.0 --port 8000`)

Now test the endpoint and have fun!

```
curl -X POST "localhost:8000/api/generate" \
  -H "Content-Type: application/json" \
  -d '{
        "prompt": "Write me a haiku about bears.",
        "max_new_tokens": 1000,
        "temperature": 0.9
      }'

```